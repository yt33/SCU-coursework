---
output:
  word_document: default
  html_document: default
---
1.
(a)
```{r}
x1 = c(3, 2, 4, 1, 2, 4, 4)
x2 = c(4, 2, 4, 4, 1, 3, 1)
colors = c("red", "red", "red", "red", "blue", "blue", "blue")
plot(x1, x2, col = colors, xlim = c(0, 5), ylim = c(0, 5))
```
(b)
```{r}
plot(x1, x2, col = colors, xlim = c(0, 5), ylim = c(0, 5))
abline(-0.5, 1)
```
(c)
Classify to Red if X1−X2−0.5 < 0, and classify to Blue otherwise
(d)
```{r}
plot(x1, x2, col = colors, xlim = c(0, 5), ylim = c(0, 5))
abline(-0.5, 1)
abline(-1, 1, lty = 2)
abline(0, 1, lty = 2)
```
(e)
The support vectors are the points (2,1), (2,2), (4,3) and (4,4).
(f)
Point (4, 1) is not support vectors, so it's ok to move it around slightly.
(g)
```{r}
plot(x1, x2, col = colors, xlim = c(0, 5), ylim = c(0, 5))
abline(-0.6, 1)
```
(h)
```{r}
plot(x1, x2, col = colors, xlim = c(0, 5), ylim = c(0, 5))
points(c(2), c(3), col = c("blue"))
```

2.
```{r}
xy <- read.table("az-5000.txt", header = TRUE)
set.seed(1)
train_ind <- sample(nrow(xy), 4000)
train_data <- xy[train_ind,]
test_data <- xy[-train_ind,]
```
(a)
```{r}
nH <- 10
n_weights <- (18 + 1)*nH + (nH + 1)*26
# Based on the rule of thumb, the approximate value of nH should be 19/10 which approximately equal to 2.
```
(b)
```{r}
num_train_y = as.numeric(train_data$char)
mat_train_y = as.matrix(num_train_y)
binary_train_y = matrix(0, nrow = nrow(mat_train_y), ncol = 26)
for(i in 1:4000){
  binary_train_y[i, num_train_y[i]] = 1
}
sum(binary_train_y == 1)
# sum is 4000
```
(c)
```{r}
library(nnet)
nns = list()
for(i in 1:20){
  nns[[i]]=nnet(char~., data = train_data, size = i, maxit = 1000)
}
```
(d)
```{r}
predicted_train_y = list()
for(i in 1:20){
  predicted_train_y[[i]] = nns[[i]]$fitted.values
}
library(Metrics)
mse_train=0
for (i in 1:20) {
  {mse_train[i] = mse(binary_train_y, predicted_train_y[[i]])}
}
```
(e)
```{r}
num_test_y = as.numeric(test_data$char)
mat_test_y = as.matrix(num_test_y)
binary_test_y = matrix(0, nrow = nrow(mat_test_y), ncol = 26)
for(i in 1:1000){
  binary_test_y[i, num_test_y[i]] = 1
}
 
predicted_test_y = list()
for(i in 1:20){
  predicted_test_y[[i]] = predict(nns[[i]], test_data)
}
mse_test = 0
for (i in 1:20) {
  {mse_test[i] = mse(binary_test_y, predicted_test_y[[i]])}
}
```
(f)
```{r}
plot(mse_train)
points(mse_test, col = "red")
mse_train[13]
mse_test[13]
# mse_train[13] = 0.005439907
# mse_test[13]  = 0.009245464
```


3.
```{r}
set.seed(1)
library(e1071)
# Loading the data "spam Data". 
spam <- read.csv("spam.csv" , TRUE)

# "number_items" is size(85% of data "spam.csv") for training data.
number_items <- 0.85 * nrow(spam)

# Variable "indexes" holds the row index values randomly selected for training data.
indexes <- sample(1:nrow(spam), number_items, replace = FALSE)

# "training_data" is the training data mapped from the index values from spam data
training_data <- spam[ indexes,]

# "test_data" is test data mapped from the negation of the index values from spam data.
test_data <- spam[- indexes,]

tuning_indexes <- sample(1:nrow(training_data), size= 500,replace=FALSE)

tuning_data <- spam[tuning_indexes, ]
```
(a)
```{r}
grid_cost <- seq(10, 100, length = 10)
grid_gamma <-seq(0.000001, 0.001, length = 10)

tuning_x <- model.matrix(type~., tuning_data)[, -1]
tuning_y <- tuning_data$type
tune.out <- tune.svm(tuning_x, tuning_y, data = tuning_data, kernel="radial", gamma = grid_gamma, cost = grid_cost)
summary(tune.out)
# The best parameters were gamma = 0.001000 and cost = 80. The best error rate was 8.2%.
# The best parameters were gamma = 0.000889 and cost = 90. The best error rate was 8.2%.
```
(b)
```{r}
training_x <- model.matrix(type~., training_data)[, -1]
training_y <- training_data$type
# for another case, gamma = 0.001000, cost = 80
train_svm <- svm(training_x, training_y, kernel = "radial", gamma = 0.000889, cost = 90)
summary(train_svm)
# There are 806 support vectors when gamma = 0.001000, cost = 80
# There are 801 support vectors when gamma = 0.000889, cost = 90
```
(c)
```{r}
test_x <- model.matrix(type~., test_data)[,-1]
test_y <- test_data$type
# for another case, gamma = 0.001000, cost = 80
test_svm <- svm(test_x, test_y, kernel = "radial", gamma = 0.000889, cost= 90)
test_pred <- predict(test_svm, test_x)
summary(test_pred)
confusion_mat <- table(test_pred, test_data$type)
accuracy <- sum(diag(confusion_mat))/sum(confusion_mat)
confusion_mat
accuracy
# When gamma = 0.001000, cost = 80:
# Confusion matrix is
#       test_pred nonspam spam
#       nonspam     399   22
#       spam         14  256
# Accuracy is 94.8% on test data.
# when gamma = 0.00889, cost = 90;
# Confusion matrix is
#       test_pred nonspam spam
#       nonspam     399   22
#       spam         14  256
# Accuracy is 94.8% on test data.
```

4.
```{r}
set.seed(1)
data = read.table("housetype_data.txt", header = TRUE, sep = ",")
data$age = as.factor(data$age)
data$sex = as.factor(data$sex)
data$ms  = as.factor(data$ms)
data$edu = as.factor(data$edu)
data$ocu = as.factor(data$ocu)
data$inc = as.factor(data$inc)
data$ba  = as.factor(data$ba)
data$di  = as.factor(data$di)
data$hhs = as.factor(data$hhs)
data$hs  = as.factor(data$hs)
data$hs2 = as.factor(data$hs2)
data$eth = as.factor(data$eth)
data$lang= as.factor(data$lang)

num = 0.9*nrow(data)
num

training = sample(nrow(data), num) 
trainingdata = data[training,]
test = -training
testdata = data[test,]
sum(is.na(data))
sum(is.na(trainingdata))
```
(a)
```{r}
library(rpart)
library(rpart.plot)
mytree = rpart(ht~., data = trainingdata, method = "class", cp = 0.0001)
prp(mytree)
```
(b)
```{r}
plotcp(mytree)
print(mytree$cptable)
which.min(mytree$cptable[,"xerror"])
```
(c)
```{r}
pruned_tree = prune(mytree, cp = 0.0014077426) 
prp(pruned_tree)
```
(d)
```{r}
plot(pruned_tree, compress = TRUE, uniform = TRUE, margin = 0.2)
text(pruned_tree, all = TRUE, pretty = 0, splits = TRUE, use.n = TRUE)
# We have pruned the tree and now it looks much smaller than the original tree. The house type strongly connects with hs (persons in your household) and connects with hhs (householder status), inc (annual income of household), and age. All of the rest predictors don't affect the house type.
```
(e)
```{r}
summary(pruned_tree)
# Yes, there are surrogate splits used in the construction of the optimal tree as shown.
# A surrogate split is used to indicate rows that have n/a in the primary splitter.
# For example, in Node number 1, hhs is the primary split, and age is the surrogate. So if hhs is missing in the observation, then age will be used to determine which tree nodes branch to go to.
```

(f)
```{r}
pretree = predict(pruned_tree, testdata, type = "class")
httest = testdata$ht
confusionM = table(pretree, httest)
sum(diag(confusionM))
sum(confusionM)
accuracy = sum(diag(confusionM))/sum(confusionM)
accuracy
# Accuracy is 73.06%
```