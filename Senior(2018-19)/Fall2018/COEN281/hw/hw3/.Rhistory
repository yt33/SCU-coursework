library(rpart)
ctree=rpart(ht~., data=trainingdata, method="class", cp=0.0001)
library(rpart)
ctree=rpart(ht~., data=trainingdata, method="class", cp=0.0001)
ctree
plotcp(ctree)
print(ctree$cptable)
which.min(ctree$cptable[,"xerror"])
pruned_tree=prune(ctree,cp=0.0014077426)
plot(pruned_tree)
plot(pruned_tree,compress=TRUE,uniform=TRUE,margin=0.2)
text(pruned_tree,all=TRUE,pretty=0,splits=TRUE,use.n=TRUE)
plot(pruned_tree,compress=TRUE,uniform=TRUE,margin=0.2)
text(pruned_tree,all=TRUE,pretty=0,splits=TRUE,use.n=TRUE)
printcp(ht.tree)
plot(pruned_tree,compress=TRUE,uniform=TRUE,margin=0.2)
text(pruned_tree,all=TRUE,pretty=0,splits=TRUE,use.n=TRUE)
prp(ht.tree)
library(rpart)
ctree=rpart(ht~., data=trainingdata, method = "class", cp = 0.0001)
plotcp(ctree)
print(ctree$cptable)
which.min(ctree$cptable[,"xerror"])
pruned_tree=prune(ctree,cp=0.0014077426)
plot(pruned_tree)
plot(pruned_tree,compress=TRUE,uniform=TRUE,margin=0.2)
text(pruned_tree,all=TRUE,pretty=0,splits=TRUE,use.n=TRUE)
library(rpart)
ctree=rpart(ht~., data=trainingdata, method = "class", cp = 0.0001)
plot(ctree)
library(rpart)
ctree=rpart(ht~., data=trainingdata, method = "class", cp = 0.0001)
rpart.plot(ctree)
library(rpart)
library(rpart.plot)
library(rpart)
ctree=rpart(ht~., data=trainingdata, method = "class", cp = 0.0001)
prp(ctree)
install.packages('rpart.plot')
library(rpart)
library(rpart.plot)
ctree=rpart(ht~., data=trainingdata, method = "class", cp = 0.0001)
prp(ctree)
pruned_tree=prune(ctree,cp=0.0014077426)
prp(pruned_tree)
prp(pruned_tree,compress=TRUE,uniform=TRUE,margin=0.2)
plot(pruned_tree,compress=TRUE,uniform=TRUE,margin=0.2)
text(pruned_tree,all=TRUE,pretty=0,splits=TRUE,use.n=TRUE)
ctree_test=rpart(ht~.,data=testdata,method="class",cp=0.0001)
plotcp(ctree_test)
print(ctree_test$cptable)
which.min(ctree_test$cptable[,"xerror"])
pretree=predict(pruned_tree,testdata,type="class")
httest=testdata$ht
confusionM=table(pretree,httest)
sum(diag(confusionM))
sum(confusionM)
accuracy=sum(diag(confusionM))/sum(confusionM)
accuracy
xy <- read.table("data/az-5000.txt",header=TRUE)
xy <- read.table("az-5000.txt",header=TRUE)
set.seed(42)
train_ind <- sample(nrow(xy),4000)
train_data <-  xy[train_ind,]
test_data <-  xy[-train_ind,]
number of weights = (input dimension + 1) * nH + (nH + 1) * output dimension
# number of weights = (input dimension + 1) * nH + (nH + 1) * output dimension
nH <- 10
n_weights <- (18 + 1)*nH + (nH + 1)*26
# number of weights = (input dimension + 1) * nH + (nH + 1) * output dimension
nH <- 10
n_weights <- (18 + 1)*nH + (nH + 1)*26
# Based on the rule of thumb, the approximate value of nH should be 19/10 which approximately equal to 2.
num_train_y = as.numeric(train_data$char)
mat_train_y = as.matrix(num_train_y)
binary_train_y = matrix(0,nrow=nrow(mat_train_y),ncol=26)
for(i in 1:4000){
binary_train_y[i,num_train_y[i]]=1
}
sum(binary_train_y==1)
library(nnet)
nns = list()
for(i in 1:20){
nns[[i]]=nnet(char~., data=train_data, size=i,maxit=1000)
}
predicted_train_y = list()
for(i in 1:20){
predicted_train_y[[i]]=nns[[i]]$fitted.values
}
library(nnet)
nns = list()
for(i in 1:20){
nns[[i]]=nnet(char~., data=train_data, size=i,maxit=1000)
}
predicted_train_y = list()
for(i in 1:20){
predicted_train_y[[i]]=nns[[i]]$fitted.values
}
library(Metrics)
predicted_train_y = list()
for(i in 1:20){
predicted_train_y[[i]]=nns[[i]]$fitted.values
}
install(Metrics)
install.packages("Metrics")
predicted_train_y = list()
for(i in 1:20){
predicted_train_y[[i]]=nns[[i]]$fitted.values
}
library(Metrics)
mse_train=0
for (i in 1:20) {
{mse_train[i]=mse(binary_train_y,predicted_train_y[[i]])}
}
num_test_y = as.numeric(test_data$char)
mat_test_y = as.matrix(num_test_y)
binary_test_y = matrix(0,nrow=nrow(mat_test_y),ncol=26)
for(i in 1:1000){
binary_test_y[i,num_test_y[i]]=1
}
predicted_test_y = list()
for(i in 1:20){
predicted_test_y[[i]]=predict(nns[[i]],test_data)
}
mse_test=0
for (i in 1:20) {
{mse_test[i]=mse(binary_test_y,predicted_test_y[[i]])}
}
plot(mse_test)
par(new=T)
plot(mse_train,col="orange",axes=F)
plot(mse_test)
par(new=T)
plot(mse_train,col="pink",axes=F)
plot(mse_test)
par(new=T)
plot(mse_train,col="pink",axes=F)
mse_train[11]
mse_test[11]
plot(mse_test)
par(new=T)
plot(mse_train,col = "pink", axes = T)
mse_train[11]
mse_test[11]
# mse_train[11] = 0.006347517
# mse_test[11] = 0.0100499
plot(mse_test)
par(new=T)
plot(mse_train,col = "red", axes = T)
mse_train[11]
mse_test[11]
# mse_train[11] = 0.006347517
# mse_test[11] = 0.0100499
plot(mse_test)
par(new=T)
plot(mse_train,col = "red", axes = F)
mse_train[11]
mse_test[11]
# mse_train[11] = 0.006347517
# mse_test[11] = 0.0100499
plot(mse_test)
par(new=T)
plot(mse_train,col = "red", axes = F)
mse_test[11]
# mse_train[11] = 0.006347517
# mse_test[11] = 0.0100499
plot(mse_train)
mse_train[11]
plot(mse_test)
par(new=T)
mse_test[11]
# mse_train[11] = 0.006347517
# mse_test[11] = 0.0100499
plot(mse_train)
point(mse_test, col = "red")
plot(mse_train)
points(mse_test, col = "red")
mse_test[11]
mse_train[11]
# mse_train[11] = 0.006347517
# mse_test[11] = 0.0100499
plot(mse_train)
points(mse_test, col = "red")
mse_train[11]
mse_test[11]
# mse_train[11] = 0.006347517
# mse_test[11] = 0.0100499
grid_cost <- seq(10,100,length=10)
grid_gamma <-seq(0.000001,0.001,length=10)
tuning_x <- model.matrix(type~.,tuning_data)[,-1]
tuning_y <- tuning_data$type
tune.out <- tune.svm(tuning_x,tuning_y,data=tuning_data, kernel="radial", gamma=grid_gamma, cost=grid_cost)
set.seed(1)
library(e1071)
# Loading the data "spam Data".
spam <- read.csv("spam.csv" , TRUE)
# "number_items" is size(85% of data "spam.csv") for training data.
number_items <- 0.85 * nrow(spam)
# Variable "indexes" holds the row index values randomly selected for training data.
indexes <- sample(1:nrow(spam), number_items, replace = FALSE)
# "training_data" is the training data mapped from the index values from spam data
training_data <- spam[ indexes,]
# "test_data" is test data mapped from the negation of the index values from spam data.
test_data <- spam[- indexes,]
tuning_indexes <- sample(1:nrow(training_data), size= 500,replace=FALSE)
tuning_data <- spam[tuning_indexes, ]
grid_cost <- seq(10,100,length=10)
grid_gamma <-seq(0.000001,0.001,length=10)
tuning_x <- model.matrix(type~.,tuning_data)[,-1]
tuning_y <- tuning_data$type
tune.out <- tune.svm(tuning_x,tuning_y,data=tuning_data, kernel="radial", gamma=grid_gamma, cost=grid_cost)
summary(tune.out)
# The best parameters were gamma = 0.001000 and cost = 80. The best error rate was 8.2%.
# The best parameters were gamma = 0.000889 and cost = 90. The best error rate was 8.2%.
x1 = c(3, 2, 4, 1, 2, 4, 4)
x2 = c(4, 2, 4, 4, 1, 3, 1)
colors = c("red", "red", "red", "red", "blue", "blue", "blue")
plot(x1, x2, col = colors, xlim = c(0, 5), ylim = c(0, 5))
plot(x1, x2, col = colors, xlim = c(0, 5), ylim = c(0, 5))
abline(-0.5, 1)
plotcp(ctree)
print(ctree$cptable)
which.min(ctree$cptable[,"xerror"])
plot(x1, x2, col = colors, xlim = c(0, 5), ylim = c(0, 5))
abline(-0.5, 1)
abline(-1, 1, lty = 2)
abline(0, 1, lty = 2)
plot(x1, x2, col = colors, xlim = c(0, 5), ylim = c(0, 5))
abline(-0.3, 1)
plot(x1, x2, col = colors, xlim = c(0, 5), ylim = c(0, 5))
abline(-0.5, 1)
plot(x1, x2, col = colors, xlim = c(0, 5), ylim = c(0, 5))
abline(-0.1, 1)
plot(x1, x2, col = colors, xlim = c(0, 5), ylim = c(0, 5))
abline(-0.2, 1)
plot(x1, x2, col = colors, xlim = c(0, 5), ylim = c(0, 5))
abline(-0.4, 1)
plot(x1, x2, col = colors, xlim = c(0, 5), ylim = c(0, 5))
abline(-0.5, 1)
plot(x1, x2, col = colors, xlim = c(0, 5), ylim = c(0, 5))
abline(-0.6, 1)
plot(x1, x2, col = colors, xlim = c(0, 5), ylim = c(0, 5))
abline(-0.7, 1)
plot(x1, x2, col = colors, xlim = c(0, 5), ylim = c(0, 5))
abline(-0.8, 1)
plot(x1, x2, col = colors, xlim = c(0, 5), ylim = c(0, 5))
abline(-0.9, 1)
plot(x1, x2, col = colors, xlim = c(0, 5), ylim = c(0, 5))
abline(-1, 1)
plot(x1, x2, col = colors, xlim = c(0, 5), ylim = c(0, 5))
abline(-0.99, 1)
plot(x1, x2, col = colors, xlim = c(0, 5), ylim = c(0, 5))
abline(-0.91, 1)
plot(x1, x2, col = colors, xlim = c(0, 5), ylim = c(0, 5))
abline(-0.96, 1)
plot(x1, x2, col = colors, xlim = c(0, 5), ylim = c(0, 5))
abline(-0.95, 1)
plot(x1, x2, col = colors, xlim = c(0, 5), ylim = c(0, 5))
abline(-0.94, 1)
plot(x1, x2, col = colors, xlim = c(0, 5), ylim = c(0, 5))
abline(-0.93, 1)
plot(x1, x2, col = colors, xlim = c(0, 5), ylim = c(0, 5))
abline(-0.06, 1)
plot(x1, x2, col = colors, xlim = c(0, 5), ylim = c(0, 5))
abline(-0.07, 1)
plot(x1, x2, col = colors, xlim = c(0, 5), ylim = c(0, 5))
abline(-0.93, 1)
# for all Î²2 in [-0.93, -0.07], we are able to sketch a hyperplane that is not the optimal separating hyperplane
plot(x1, x2, col = colors, xlim = c(0, 5), ylim = c(0, 5))
abline(-0.6, 1)
plot(x1, x2, col = colors, xlim = c(0, 5), ylim = c(0, 5))
points(c(2), c(3), col = c("blue"))
xy <- read.table("az-5000.txt",header=TRUE)
set.seed(1)
train_ind <- sample(nrow(xy),4000)
train_data <-  xy[train_ind,]
test_data <-  xy[-train_ind,]
nH <- 10
n_weights <- (18 + 1)*nH + (nH + 1)*26
# Based on the rule of thumb, the approximate value of nH should be 19/10 which approximately equal to 2.
num_train_y = as.numeric(train_data$char)
mat_train_y = as.matrix(num_train_y)
binary_train_y = matrix(0,nrow=nrow(mat_train_y),ncol=26)
for(i in 1:4000){
binary_train_y[i,num_train_y[i]]=1
}
sum(binary_train_y==1)
# sum is 4000
xy <- read.table("az-5000.txt", header = TRUE)
set.seed(1)
train_ind <- sample(nrow(xy), 4000)
train_data <- xy[train_ind,]
test_data <- xy[-train_ind,]
nH <- 10
n_weights <- (18 + 1)*nH + (nH + 1)*26
# Based on the rule of thumb, the approximate value of nH should be 19/10 which approximately equal to 2.
num_train_y = as.numeric(train_data$char)
mat_train_y = as.matrix(num_train_y)
binary_train_y = matrix(0, nrow = nrow(mat_train_y), ncol = 26)
for(i in 1:4000){
binary_train_y[i, num_train_y[i]] = 1
}
sum(binary_train_y == 1)
# sum is 4000
library(nnet)
nns = list()
for(i in 1:20){
nns[[i]]=nnet(char~., data = train_data, size = i, maxit = 1000)
}
predicted_train_y = list()
for(i in 1:20){
predicted_train_y[[i]] = nns[[i]]$fitted.values
}
library(Metrics)
mse_train=0
for (i in 1:20) {
{mse_train[i] = mse(binary_train_y, predicted_train_y[[i]])}
}
num_test_y = as.numeric(test_data$char)
mat_test_y = as.matrix(num_test_y)
binary_test_y = matrix(0, nrow = nrow(mat_test_y), ncol = 26)
for(i in 1:1000){
binary_test_y[i, num_test_y[i]] = 1
}
predicted_test_y = list()
for(i in 1:20){
predicted_test_y[[i]] = predict(nns[[i]], test_data)
}
mse_test = 0
for (i in 1:20) {
{mse_test[i] = mse(binary_test_y, predicted_test_y[[i]])}
}
plot(mse_train)
points(mse_test, col = "red")
mse_train[11]
mse_test[11]
# mse_train[11] = 0.006347517
# mse_test[11] = 0.0100499
plot(mse_train)
points(mse_test, col = "red")
mse_train[13]
mse_test[13]
# mse_train[13] = 0.007293846
# mse_test[13] = 0.01033311
set.seed(1)
library(e1071)
# Loading the data "spam Data".
spam <- read.csv("spam.csv" , TRUE)
# "number_items" is size(85% of data "spam.csv") for training data.
number_items <- 0.85 * nrow(spam)
# Variable "indexes" holds the row index values randomly selected for training data.
indexes <- sample(1:nrow(spam), number_items, replace = FALSE)
# "training_data" is the training data mapped from the index values from spam data
training_data <- spam[ indexes,]
# "test_data" is test data mapped from the negation of the index values from spam data.
test_data <- spam[- indexes,]
tuning_indexes <- sample(1:nrow(training_data), size= 500,replace=FALSE)
tuning_data <- spam[tuning_indexes, ]
grid_cost <- seq(10, 100, length = 10)
grid_gamma <-seq(0.000001, 0.001, length = 10)
tuning_x <- model.matrix(type~., tuning_data)[, -1]
tuning_y <- tuning_data$type
tune.out <- tune.svm(tuning_x, tuning_y, data = tuning_data, kernel="radial", gamma = grid_gamma, cost = grid_cost)
summary(tune.out)
# The best parameters were gamma = 0.001000 and cost = 80. The best error rate was 8.2%.
# The best parameters were gamma = 0.000889 and cost = 90. The best error rate was 8.2%.
training_x <- model.matrix(type~., training_data)[,-1]
training_y <- training_data$type
# for another case, gamma = 0.001000, cost = 80
train_svm <- svm(training_x,training_y,kernel="radial", gamma = 0.000889, cost = 90)
summary(train_svm)
# There are 806 support vectors when gamma = 0.001000, cost = 80
# There are 801 support vectors when gamma = 0.000889, cost = 90
training_x <- model.matrix(type~., training_data)[, -1]
training_y <- training_data$type
# for another case, gamma = 0.001000, cost = 80
train_svm <- svm(training_x, training_y, kernel = "radial", gamma = 0.000889, cost = 90)
summary(train_svm)
# There are 806 support vectors when gamma = 0.001000, cost = 80
# There are 801 support vectors when gamma = 0.000889, cost = 90
test_x <- model.matrix(type~., test_data)[,-1]
test_y <- test_data$type
# for another case, gamma = 0.001000, cost = 80
test_svm <- svm(test_x, test_y, kernel = "radial", gamma = 0.000889, cost= 90)
test_pred <- predict(test_svm, test_x)
summary(test_pred)
confusion_mat <- table(test_pred, test_data$type)
accuracy <- sum(diag(confusion_mat))/sum(confusion_mat)
confusion_mat
accuracy
# When gamma = 0.001000, cost = 80:
# Confusion matrix is
#       test_pred nonspam spam
#       nonspam     399   22
#       spam         14  256
# Accuracy is 94.8% on test data.
# when gamma = 0.00889, cost = 90;
# Confusion matrix is
#       test_pred nonspam spam
#       nonspam     399   22
#       spam         14  256
# Accuracy is 94.8% on test data.
set.seed(1)
data = read.table("housetype_data.txt", header = TRUE, sep = ",")
data$age = as.factor(data$age)
data$sex = as.factor(data$sex)
data$ms  = as.factor(data$ms)
data$edu = as.factor(data$edu)
data$ocu = as.factor(data$ocu)
data$inc = as.factor(data$inc)
data$ba  = as.factor(data$ba)
data$di  = as.factor(data$di)
data$hhs = as.factor(data$hhs)
data$hs  = as.factor(data$hs)
data$hs2 = as.factor(data$hs2)
data$eth = as.factor(data$eth)
data$lang= as.factor(data$lang)
num = 0.9*nrow(data)
num
training = sample(nrow(data), num)
trainingdata = data[training,]
test = -training
testdata = data[test,]
sum(is.na(data))
sum(is.na(trainingdata))
library(rpart)
library(rpart.plot)
mytree = rpart(ht~., data = trainingdata, method = "class", cp = 0.0001)
prp(mytree)
plotcp(mytree)
print(mytree$cptable)
which.min(mytree$cptable[,"xerror"])
pruned_tree = prune(mytree, cp = 0.0014077426)
prp(pruned_tree)
plot(pruned_tree, compress = TRUE, uniform = TRUE, margin = 0.2)
text(pruned_tree, all = TRUE, pretty = 0, splits = TRUE, use.n = TRUE)
# We have pruned the tree and now it looks much smaller than the original tree. The house type strongly connects with hs (persons in your household) and connects with hhs (householder status), inc (annual income of household), and age. All of the rest predictors don't affect the house type.
pretree=predict(pruned_tree, testdata, type="class")
httest=testdata$ht
confusionM=table(pretree,httest)
sum(diag(confusionM))
sum(confusionM)
accuracy=sum(diag(confusionM))/sum(confusionM)
accuracy
# Accuracy is 74.1%
summary(pruned_tree)
plot(pruned_tree, branch = 0.4, uniform = T)
text(pruned_tree, use.n = F, digits = 3, pretty = 0)
#plot(pruned_tree, compress = TRUE, uniform = TRUE, margin = 0.2)
#text(pruned_tree, all = TRUE, pretty = 0, splits = TRUE, use.n = TRUE)
# We have pruned the tree and now it looks much smaller than the original tree. The house type strongly connects with hs (persons in your household) and connects with hhs (householder status), inc (annual income of household), and age. All of the rest predictors don't affect the house type.
plot(pruned_tree, branch = 0.6, uniform = T)
text(pruned_tree, use.n = F, digits = 3, pretty = 0)
#plot(pruned_tree, compress = TRUE, uniform = TRUE, margin = 0.2)
#text(pruned_tree, all = TRUE, pretty = 0, splits = TRUE, use.n = TRUE)
# We have pruned the tree and now it looks much smaller than the original tree. The house type strongly connects with hs (persons in your household) and connects with hhs (householder status), inc (annual income of household), and age. All of the rest predictors don't affect the house type.
plot(pruned_tree, branch = 0.4, uniform = T)
text(pruned_tree, use.n = F, digits = 3, pretty = 0)
#plot(pruned_tree, compress = TRUE, uniform = TRUE, margin = 0.2)
#text(pruned_tree, all = TRUE, pretty = 0, splits = TRUE, use.n = TRUE)
# We have pruned the tree and now it looks much smaller than the original tree. The house type strongly connects with hs (persons in your household) and connects with hhs (householder status), inc (annual income of household), and age. All of the rest predictors don't affect the house type.
plot(pruned_tree, branch = 0.4, uniform = T)
text(pruned_tree, use.n = F, digits = 1, pretty = 0)
#plot(pruned_tree, compress = TRUE, uniform = TRUE, margin = 0.2)
#text(pruned_tree, all = TRUE, pretty = 0, splits = TRUE, use.n = TRUE)
# We have pruned the tree and now it looks much smaller than the original tree. The house type strongly connects with hs (persons in your household) and connects with hhs (householder status), inc (annual income of household), and age. All of the rest predictors don't affect the house type.
plot(pruned_tree, branch = 0.4, uniform = T)
text(pruned_tree, use.n = F, digits = 1, pretty = 1)
#plot(pruned_tree, compress = TRUE, uniform = TRUE, margin = 0.2)
#text(pruned_tree, all = TRUE, pretty = 0, splits = TRUE, use.n = TRUE)
# We have pruned the tree and now it looks much smaller than the original tree. The house type strongly connects with hs (persons in your household) and connects with hhs (householder status), inc (annual income of household), and age. All of the rest predictors don't affect the house type.
plot(pruned_tree, branch = 0.4, uniform = T)
text(pruned_tree, use.n = F, digits = 1, pretty = 1)
#plot(pruned_tree, compress = TRUE, uniform = TRUE, margin = 0.2)
#text(pruned_tree, all = TRUE, pretty = 0, splits = TRUE, use.n = TRUE)
# We have pruned the tree and now it looks much smaller than the original tree. The house type strongly connects with hs (persons in your household) and connects with hhs (householder status), inc (annual income of household), and age. All of the rest predictors don't affect the house type.
plot(pruned_tree, branch = 0.4, uniform = T)
text(pruned_tree, use.n = F, digits = 1, pretty = 10)
#plot(pruned_tree, compress = TRUE, uniform = TRUE, margin = 0.2)
#text(pruned_tree, all = TRUE, pretty = 0, splits = TRUE, use.n = TRUE)
# We have pruned the tree and now it looks much smaller than the original tree. The house type strongly connects with hs (persons in your household) and connects with hhs (householder status), inc (annual income of household), and age. All of the rest predictors don't affect the house type.
plot(pruned_tree, branch = 0.4, uniform = T)
text(pruned_tree, use.n = F, digits = 1, pretty = 0)
#plot(pruned_tree, compress = TRUE, uniform = TRUE, margin = 0.2)
#text(pruned_tree, all = TRUE, pretty = 0, splits = TRUE, use.n = TRUE)
# We have pruned the tree and now it looks much smaller than the original tree. The house type strongly connects with hs (persons in your household) and connects with hhs (householder status), inc (annual income of household), and age. All of the rest predictors don't affect the house type.
plot(pruned_tree, compress = TRUE, uniform = TRUE, margin = 0.2)
text(pruned_tree, all = TRUE, pretty = 0, splits = TRUE, use.n = TRUE)
# We have pruned the tree and now it looks much smaller than the original tree. The house type strongly connects with hs (persons in your household) and connects with hhs (householder status), inc (annual income of household), and age. All of the rest predictors don't affect the house type.
pretree = predict(pruned_tree, testdata, type = "class")
httest = testdata$ht
confusionM = table(pretree, httest)
sum(diag(confusionM))
sum(confusionM)
accuracy = sum(diag(confusionM))/sum(confusionM)
accuracy
# Accuracy is 73.06%
x1 = c(3, 2, 4, 1, 2, 4, 4)
x2 = c(4, 2, 4, 4, 1, 3, 1)
colors = c("red", "red", "red", "red", "blue", "blue", "blue")
plot(x1, x2, col = colors, xlim = c(0, 5), ylim = c(0, 5))
plot(x1, x2, col = colors, xlim = c(0, 5), ylim = c(0, 5))
abline(-0.5, 1)
plot(x1, x2, col = colors, xlim = c(0, 5), ylim = c(0, 5))
abline(-0.5, 1)
abline(-1, 1, lty = 2)
abline(0, 1, lty = 2)
plot(x1, x2, col = colors, xlim = c(0, 5), ylim = c(0, 5))
abline(-0.6, 1)
plot(x1, x2, col = colors, xlim = c(0, 5), ylim = c(0, 5))
points(c(2), c(3), col = c("blue"))
questionr:::iorder()
